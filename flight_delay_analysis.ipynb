{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "165771bd",
   "metadata": {},
   "source": [
    "# Airline Delay Analysis\n",
    "\n",
    "#### This coursework uses the U.S. Department of Transportationâ€™s Airline On-Time Statistics and Delay Causes dataset, which provides monthly records on domestic flights operated by major U.S. carriers. The dataset includes counts of on-time, delayed, canceled, and diverted flights, along with detailed delay-cause categories such as Air Carrier, Extreme Weather, NAS (National Aviation System), Late-Arriving Aircraft, and Security.\n",
    "\n",
    "#### Using this dataset of over 400,000 flight records (2003-2025), the coursework develops a full machine learning pipeline for analyzing and predicting airline delays. The goal is to perform exploratory data analysis, engineer meaningful features, train regression and classification models, and deploy an interactive Streamlit application for real-time delay predictions. The project also aims to extract business insights that airlines and airports can use to improve operational efficiency and reduce delays.\n",
    "\n",
    "## Student ID: 00016395\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658d7080",
   "metadata": {},
   "source": [
    "# Loading the dataset, identify shape and overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75757a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Airline_Delay_Cause.csv')\n",
    "print(\"Dataset loaded sucessfully!!!\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84279626",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f4ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Dataset First 10 rows: \")\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad455fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking datatypes: ')\n",
    "df.info()\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary for numerical columns:')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc87c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary for categorical columns:')\n",
    "df.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce2fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking missing values:')\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78461a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking for duplicates:')\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b09bd78",
   "metadata": {},
   "source": [
    "### Distribution of arrival delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738be21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(df['arr_delay'], bins=100, kde=True)\n",
    "plt.title('Distribution of Arrival Delay')\n",
    "plt.xlabel('Minutes')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd68a4",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba56d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[num_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4565b72b",
   "metadata": {},
   "source": [
    "#### Avarage delay by type(cause of delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b1ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_types = ['carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "delay_means = df[delay_types].mean()\n",
    "plt.bar(delay_means.index, delay_means.values)\n",
    "plt.title('Average Delay by Type')\n",
    "plt.xlabel('Delay Type')\n",
    "plt.ylabel('Average Minutes')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad622dab",
   "metadata": {},
   "source": [
    "#### Delays by carier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72976f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "top_carriers = df['carrier'].value_counts().nlargest(10).index\n",
    "df_top = df[df['carrier'].isin(top_carriers)]\n",
    "sns.boxplot(x='carrier', y='arr_delay', data=df_top)\n",
    "plt.title('Arrival Delay Distribution by Carrier (Top 10)')\n",
    "plt.xlabel('Carrier Code')\n",
    "plt.ylabel('Delay Minutes')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3688ea37",
   "metadata": {},
   "source": [
    "#### Avarage Delay by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6391afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_delay = df.groupby('month')['arr_delay'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(monthly_delay.index, monthly_delay.values, color='orange', alpha=0.7)\n",
    "plt.title('Average Delay by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Delay (minutes)')\n",
    "plt.xticks(range(1, 13))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ac82b",
   "metadata": {},
   "source": [
    "#### Cancellation and diversion graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd81685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "cancellation_rate = df.groupby('month')['arr_cancelled'].mean()\n",
    "plt.bar(cancellation_rate.index, cancellation_rate.values)\n",
    "plt.title('Cancellation Rate by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Cancellation Rate')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "diversion_rate = df.groupby('month')['arr_diverted'].mean()\n",
    "plt.bar(diversion_rate.index, diversion_rate.values)\n",
    "plt.title('Diversion Rate by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Diversion Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5290303e",
   "metadata": {},
   "source": [
    "# Data Preperation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8cd648",
   "metadata": {},
   "source": [
    "### Copy of dataset for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff1313",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3894dc",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb031ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Missing values before handling:\")\n",
    "print(df_clean.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176ed41",
   "metadata": {},
   "source": [
    "For better performance of the upcoming models i have chosen filling missing values rather than dropping them, and followed strategy below:\n",
    "- for delay minutes columns filling with 0(no delays)\n",
    "- for categorical columns filled with 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313acc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_minutes_cols = ['carrier_delay', 'weather_delay', 'nas_delay', \n",
    "                      'security_delay', 'late_aircraft_delay', 'arr_delay', 'arr_cancelled', 'arr_diverted']\n",
    "for col in delay_minutes_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = df_clean[col].fillna(0)\n",
    "        \n",
    "print('Data filled with 0 for delay minutes columns')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b297758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_cols = ['carrier_ct', 'weather_ct', 'nas_ct', 'security_ct', \n",
    "              'late_aircraft_ct', 'arr_del15', 'arr_flights']\n",
    "for col in count_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = df_clean[col].fillna(0)\n",
    "        \n",
    "print('Data filled with 0 for count columns')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a5e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['carrier', 'carrier_name', 'airport', 'airport_name']\n",
    "for col in categorical_cols:\n",
    "    if col in df_clean.columns and df_clean[col].isnull().any():\n",
    "        df_clean[col] = df_clean[col].fillna('Unknown')\n",
    "        \n",
    "print('Data filled with Unknown for categorical columns')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b7790",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nMissing values after handling:\")\n",
    "print(df_clean.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a873e3a",
   "metadata": {},
   "source": [
    "### Checking for data with errors (eg. negative flights, delays more than flights )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a81ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'arr_flights' in df_clean.columns:\n",
    "    negative_flights = (df_clean['arr_flights'] < 0).sum()\n",
    "    if negative_flights > 0:\n",
    "        print(f\"Found {negative_flights} rows with negative flight count. Setting to 0.\")\n",
    "        df_clean['arr_flights'] = df_clean['arr_flights'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all(col in df_clean.columns for col in ['arr_del15', 'arr_flights']):\n",
    "    impossible = (df_clean['arr_del15'] > df_clean['arr_flights']).sum()\n",
    "    if impossible > 0:\n",
    "        print(f\"Found {impossible} rows where delayed flights > total flights. Correcting...\")\n",
    "        df_clean['arr_del15'] = df_clean[['arr_del15', 'arr_flights']].min(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1aafe2",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = ((data[column] < lower_bound) | (data[column] > upper_bound)).sum()\n",
    "    \n",
    "    if outliers > 0:\n",
    "        data[column] = np.clip(data[column], lower_bound, upper_bound)\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "outlier_cols = ['arr_delay', 'carrier_delay', 'weather_delay', 'nas_delay', 'late_aircraft_delay']\n",
    "\n",
    "for col in outlier_cols:\n",
    "    if col in df_clean.columns:\n",
    "        outliers_count = handle_outliers(df_clean, col)\n",
    "        if outliers_count > 0:\n",
    "            print(f\"  - {col}: {outliers_count} outliers capped\")\n",
    "\n",
    "print(\"Outliers handled using IQR method!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52b316",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71476d37",
   "metadata": {},
   "source": [
    "New binary target for classifying whether there is delay or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b23f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['delay_rate'] = df_clean['arr_del15'] / df_clean['arr_flights'].replace(0, 1)\n",
    "print(\"Delay rate statistics:\")\n",
    "print(df_clean['delay_rate'].describe())\n",
    "\n",
    "df_clean['high_delay_month'] = (df_clean['delay_rate'] > 0.25).astype(int)\n",
    "print(\"Class distribution:\")\n",
    "print(df_clean['high_delay_month'].value_counts(normalize=True) * 100)\n",
    "\n",
    "delay_cause_cols = ['carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "total_delay_minutes = df_clean[delay_cause_cols].sum(axis=1)\n",
    "\n",
    "for col in delay_cause_cols:\n",
    "    percentage_col = col.replace('_delay', '_percentage')\n",
    "    df_clean[percentage_col] = np.where(\n",
    "        total_delay_minutes > 0,\n",
    "        df_clean[col] / total_delay_minutes * 100,\n",
    "        0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['quarter'] = ((df_clean['month'] - 1) // 3) + 1\n",
    "df_clean['is_winter'] = df_clean['month'].isin([12, 1, 2]).astype(int)\n",
    "df_clean['is_summer'] = df_clean['month'].isin([6, 7, 8]).astype(int)\n",
    "df_clean['is_peak_travel'] = df_clean['month'].isin([6, 7, 8, 11, 12]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523924af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['flights_per_day'] = df_clean['arr_flights'] / 30  # Approximate daily flights\n",
    "df_clean['cancellation_rate'] = df_clean['arr_cancelled'] / df_clean['arr_flights'].replace(0, 1)\n",
    "df_clean['total_disruptions'] = df_clean['arr_cancelled'] + df_clean['arr_diverted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f295313",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_stats = df_clean.groupby('airport').agg({\n",
    "    'arr_flights': ['sum', 'mean'],  # Use flight volume, not delay rate\n",
    "    'arr_cancelled': 'sum',\n",
    "    'arr_diverted': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "airport_stats.columns = ['airport_total_flights', 'airport_avg_flights', 'airport_total_cancelled', 'airport_total_diverted']\n",
    "\n",
    "carrier_stats = df_clean.groupby('carrier').agg({\n",
    "    'arr_flights': ['sum', 'mean'],  # Use flight volume, not delay rate\n",
    "    'arr_cancelled': 'sum', \n",
    "    'arr_diverted': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names  \n",
    "carrier_stats.columns = ['carrier_total_flights', 'carrier_avg_flights', 'carrier_total_cancelled', 'carrier_total_diverted']\n",
    "\n",
    "# Merge back\n",
    "df_clean = df_clean.merge(airport_stats, left_on='airport', right_index=True, how='left')\n",
    "df_clean = df_clean.merge(carrier_stats, left_on='carrier', right_index=True, how='left')\n",
    "\n",
    "print(\"Feature engineering completed without data leakage!\")\n",
    "print(f\"Dataset shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a6528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data leakage columns and redundant features\n",
    "leakage_cols = [\n",
    "    # Direct delay indicators (CRITICAL TO REMOVE)\n",
    "    'arr_del15', 'arr_delay',\n",
    "    'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay',\n",
    "    'carrier_ct', 'weather_ct', 'nas_ct', 'security_ct', 'late_aircraft_ct',\n",
    "    \n",
    "    # Target-derived features (CRITICAL TO REMOVE)\n",
    "    'delay_rate',  # Used to create target\n",
    "    'carrier_percentage', 'weather_percentage', 'nas_percentage', 'security_percentage', 'late_aircraft_percentage',\n",
    "    \n",
    "    # Redundant features\n",
    "    'carrier_name', 'airport_name'\n",
    "]\n",
    "\n",
    "# Keep percentage columns for cause prediction\n",
    "clean_features = [col for col in df_clean.columns if col not in leakage_cols + ['high_delay_month']]\n",
    "df_model = df_clean[clean_features + ['high_delay_month']].copy()\n",
    "\n",
    "print(f\"Removed {len(leakage_cols)} leakage columns\")\n",
    "print(f\"Clean dataset shape: {df_model.shape}\")\n",
    "print(\"Remaining features:\", clean_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f1b30",
   "metadata": {},
   "source": [
    "### Feature and Target Prepation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee6b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_model.drop('high_delay_month', axis=1)\n",
    "y = df_model['high_delay_month']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(y.value_counts(normalize=True) * 100)\n",
    "\n",
    "# Check class imbalance ratio\n",
    "imbalance_ratio = y.value_counts()\n",
    "minority_ratio = imbalance_ratio[1] / imbalance_ratio[0]\n",
    "print(f\"Class imbalance ratio: {imbalance_ratio[0]}:{imbalance_ratio[1]} (No delay:Delay)\")\n",
    "print(f\"Minority class is {minority_ratio:.2%} of majority class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd117f",
   "metadata": {},
   "source": [
    "### Class Weight Method for Imbalance handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PRACTICE: Proper class weight handling\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Calculate class weights\n",
    "classes = np.unique(y)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "print(f\"Calculated class weights: {class_weight_dict}\")\n",
    "print(f\"Minority class weight: {class_weight_dict[1]:.3f}\")\n",
    "print(f\"Majority class weight: {class_weight_dict[0]:.3f}\")\n",
    "print(f\"Weight ratio: {class_weight_dict[1]/class_weight_dict[0]:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ac0bb",
   "metadata": {},
   "source": [
    "### Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b7f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "label_encoders = {}\n",
    "\n",
    "X_encoded = X.copy()\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"Encoded {col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "print(\"Categorical encoding completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f7ed6a",
   "metadata": {},
   "source": [
    "### Scaling the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc4f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = X_encoded.select_dtypes(include=[np.number]).columns\n",
    "X_scaled = X_encoded.copy()\n",
    "X_scaled[numerical_cols] = scaler.fit_transform(X_encoded[numerical_cols])\n",
    "\n",
    "print(\"Feature scaling completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b772e1ef",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772fef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PRACTICE: Stratified split to maintain class distribution\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # CRITICAL: Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Verify stratification worked\n",
    "print(\"\\nClass distribution verification:\")\n",
    "print(\"Training set:\")\n",
    "train_dist = pd.Series(y_train).value_counts(normalize=True) * 100\n",
    "for idx, val in train_dist.items():\n",
    "    status = \"High Delay\" if idx == 1 else \"No High Delay\"\n",
    "    print(f\"  {status}: {val:.2f}%\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "test_dist = pd.Series(y_test).value_counts(normalize=True) * 100\n",
    "for idx, val in test_dist.items():\n",
    "    status = \"High Delay\" if idx == 1 else \"No High Delay\"\n",
    "    print(f\"  {status}: {val:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6de210",
   "metadata": {},
   "source": [
    "# ML Models - Algorithms training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "best_params = {}\n",
    "training_times = {}\n",
    "cv_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15e3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "\n",
    "lr_param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "lr_base = LogisticRegression(class_weight='balanced', random_state=42, max_iter=2000)\n",
    "\n",
    "start_time = time.time()\n",
    "lr_grid = GridSearchCV(lr_base, lr_param_grid, cv=3, scoring='f1', n_jobs=-1, verbose=0)\n",
    "lr_grid.fit(X_train, y_train)\n",
    "lr_time = time.time() - start_time\n",
    "\n",
    "models['Logistic Regression'] = lr_grid.best_estimator_\n",
    "best_params['Logistic Regression'] = lr_grid.best_params_\n",
    "training_times['Logistic Regression'] = lr_time\n",
    "\n",
    "# Cross-validation score\n",
    "lr_cv_scores = cross_val_score(lr_grid.best_estimator_, X_train, y_train, cv=5, scoring='f1')\n",
    "cv_scores['Logistic Regression'] = lr_cv_scores.mean()\n",
    "\n",
    "print(f\"Best parameters: {lr_grid.best_params_}\")\n",
    "print(f\"Training time: {lr_time:.2f} seconds\")\n",
    "print(f\"CV F1-score: {lr_cv_scores.mean():.4f} (+/- {lr_cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9235ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "print('Training Random Forest...')\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [2, 5]\n",
    "}\n",
    "\n",
    "rf_base = RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "rf_grid = GridSearchCV(\n",
    "    rf_base, rf_param_grid,\n",
    "    cv=3, scoring='f1',\n",
    "    n_jobs=-1, verbose=0\n",
    ")\n",
    "rf_grid.fit(X_train, y_train)\n",
    "rf_time = time.time() - start_time\n",
    "\n",
    "models['Random Forest'] = rf_grid.best_estimator_\n",
    "best_params['Random Forest'] = rf_grid.best_params_\n",
    "training_times['Random Forest'] = rf_time\n",
    "\n",
    "# Cross-validation score\n",
    "rf_cv_scores = cross_val_score(rf_grid.best_estimator_, X_train, y_train, cv=5, scoring='f1')\n",
    "cv_scores['Random Forest'] = rf_cv_scores.mean()\n",
    "\n",
    "print(f\"Best parameters: {rf_grid.best_params_}\")\n",
    "print(f\"Training time: {rf_time:.2f} seconds\")\n",
    "print(f\"CV F1-score: {rf_cv_scores.mean():.4f} (+/- {rf_cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# knn_param_grid = {\n",
    "#     'n_neighbors': [3, 5, 7, 9],\n",
    "#     'weights': ['uniform', 'distance'],\n",
    "#     'p': [1, 2]  # Manhattan vs Euclidean distance\n",
    "# }\n",
    "\n",
    "# knn_base = KNeighborsClassifier()\n",
    "\n",
    "# start_time = time.time()\n",
    "# knn_grid = GridSearchCV(\n",
    "#     knn_base, knn_param_grid,\n",
    "#     cv=3, scoring='f1',\n",
    "#     n_jobs=-1, verbose=0\n",
    "# )\n",
    "# knn_grid.fit(X_train, y_train)\n",
    "# knn_time = time.time() - start_time\n",
    "\n",
    "# models['K-Nearest Neighbors'] = knn_grid.best_estimator_\n",
    "# best_params['K-Nearest Neighbors'] = knn_grid.best_params_\n",
    "# training_times['K-Nearest Neighbors'] = knn_time\n",
    "\n",
    "# # Cross-validation score\n",
    "# knn_cv_scores = cross_val_score(knn_grid.best_estimator_, X_train, y_train, cv=5, scoring='f1')\n",
    "# cv_scores['K-Nearest Neighbors'] = knn_cv_scores.mean()\n",
    "\n",
    "# print(f\"Best parameters: {knn_grid.best_params_}\")\n",
    "# print(f\"Training time: {knn_time:.2f} seconds\")\n",
    "# print(f\"CV F1-score: {knn_cv_scores.mean():.4f} (+/- {knn_cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ddafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "print('Training Naive Bayes...')\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "start_time = time.time()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_time = time.time() - start_time\n",
    "\n",
    "models['Naive Bayes'] = nb_model\n",
    "best_params['Naive Bayes'] = 'No Hyperparameters'\n",
    "training_times['Naive Bayes'] = nb_time\n",
    "\n",
    "# Cross-validation score\n",
    "nb_cv_scores = cross_val_score(nb_model, X_train, y_train, cv=5, scoring='f1')\n",
    "cv_scores['Naive Bayes'] = nb_cv_scores.mean()\n",
    "\n",
    "print(f\"Training time: {nb_time:.2f} seconds\")\n",
    "print(f\"CV F1-score: {nb_cv_scores.mean():.4f} (+/- {nb_cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14269c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "print('Training Decision Tree...')\n",
    "\n",
    "dt_param_grid = {\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [5, 10, 20],\n",
    "    'min_samples_leaf': [2, 5, 10]\n",
    "}\n",
    "\n",
    "dt_base = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "start_time = time.time()\n",
    "dt_grid = GridSearchCV(\n",
    "    dt_base, dt_param_grid,\n",
    "    cv=3, scoring='f1',\n",
    "    n_jobs=-1, verbose=0)\n",
    "dt_grid.fit(X_train, y_train)\n",
    "dt_time = time.time() - start_time\n",
    "\n",
    "models['Decision Tree'] = dt_grid.best_estimator_\n",
    "best_params['Decision Tree'] = dt_grid.best_params_\n",
    "training_times['Decision Tree'] = dt_time\n",
    "\n",
    "# Cross-validation score\n",
    "dt_cv_scores = cross_val_score(dt_grid.best_estimator_, X_train, y_train, cv=5, scoring='f1')\n",
    "cv_scores['Decision Tree'] = dt_cv_scores.mean()\n",
    "\n",
    "print(f\"Best parameters: {dt_grid.best_params_}\")\n",
    "print(f\"Training time: {dt_time:.2f} seconds\")\n",
    "print(f\"CV F1-score: {dt_cv_scores.mean():.4f} (+/- {dt_cv_scores.std() * 2:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7924bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for model_name in models.keys():\n",
    "    print(f\"{model_name:20} | F1: {cv_scores[model_name]:.4f} | Time: {training_times[model_name]:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064796ed",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37deed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate all models on test set\n",
    "test_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    test_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(test_results.keys(), key=lambda k: test_results[k]['f1_score'])\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Best F1-Score: {test_results[best_model_name]['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec2dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison Visualization\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(test_results.keys()),\n",
    "    'Accuracy': [results['accuracy'] for results in test_results.values()],\n",
    "    'F1-Score': [results['f1_score'] for results in test_results.values()],\n",
    "    'ROC-AUC': [results['roc_auc'] for results in test_results.values()],\n",
    "    'Training Time (s)': [training_times[model] for model in test_results.keys()]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0,0].bar(comparison_df['Model'], comparison_df['Accuracy'])\n",
    "axes[0,0].set_title('Model Accuracy Comparison')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# F1-Score comparison\n",
    "axes[0,1].bar(comparison_df['Model'], comparison_df['F1-Score'])\n",
    "axes[0,1].set_title('Model F1-Score Comparison')\n",
    "axes[0,1].set_ylabel('F1-Score')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ROC-AUC comparison\n",
    "axes[1,0].bar(comparison_df['Model'], comparison_df['ROC-AUC'])\n",
    "axes[1,0].set_title('Model ROC-AUC Comparison')\n",
    "axes[1,0].set_ylabel('ROC-AUC')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Training Time comparison\n",
    "axes[1,1].bar(comparison_df['Model'], comparison_df['Training Time (s)'])\n",
    "axes[1,1].set_title('Model Training Time Comparison')\n",
    "axes[1,1].set_ylabel('Training Time (seconds)')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99706f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, results in test_results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, results['y_pred_proba'])\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {results['roc_auc']:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - All Models')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185c4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation for best model\n",
    "best_model = models[best_model_name]\n",
    "y_pred_best = test_results[best_model_name]['y_pred']\n",
    "\n",
    "print(f\"\\nDetailed Evaluation for {best_model_name}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['No High Delay', 'High Delay']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No High Delay', 'High Delay'],\n",
    "            yticklabels=['No High Delay', 'High Delay'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
